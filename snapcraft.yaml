name: microk8s
version-script: |
  . build-scripts/set-env-variables.sh > /dev/null
  echo $KUBE_VERSION
version: "latest"
summary: Kubernetes for workstations and appliances
description: |-
 MicroK8s is a small, fast, secure, single node Kubernetes that installs on
 just about any Linux box. Use it for offline development, prototyping, 
 testing, or use it on a VM as a small, cheap, reliable k8s for CI/CD. It's 
 also a great k8s for appliances - develop your IoT apps for k8s and deploy 
 them to MicroK8s on your boxes.

grade: devel
confinement: strict

plugs:
  docker-privileged:
    interface: docker-support
    privileged-containers: true
  k8s-kubelet:
    interface: kubernetes-support
    flavor: kubelet
  k8s-kubeproxy:
    interface: kubernetes-support
    flavor: kubeproxy
  dot-kube:
    interface: personal-files
    write:
      - $HOME/.kube

hooks:
  configure:
    plugs:
    - dot-kube
  install:
    plugs: []
  remove:
    plugs:
    - k8s-kubelet
    - mount-observe

apps:
  daemon-etcd:
    command: run-etcd-with-args
    daemon: simple
    plugs:
    - network-bind
  daemon-flanneld:
    command: run-flanneld-with-args
    daemon: simple
    plugs:
    - network-bind
    - network-control
    - firewall-control
    - docker-support
    - docker-privileged
  daemon-containerd:
    command: run-containerd-with-args
    daemon: simple
    plugs:
    - network-bind
    - docker-support
    - docker-privileged
    - firewall-control
    - kernel-module-control
    - network-control
    - mount-observe
  daemon-apiserver:
    command: run-with-config-args kube-apiserver
    daemon: simple
    plugs:
    - network-bind
    - network-observe
  daemon-apiserver-kicker:
    command: apiservice-kicker
    daemon: simple
    plugs:
    - network-bind
    - network-observe
  daemon-cluster-agent:
    command: run-cluster-agent-with-args
    daemon: simple
    plugs:
    - network-bind
    - network-observe
  daemon-controller-manager:
    command: run-with-config-args kube-controller-manager
    daemon: simple
    plugs:
    - network-bind
    - docker-support
    - docker-privileged
    - firewall-control
    - kernel-module-control
    - network-control
  daemon-scheduler:
    command: run-with-config-args kube-scheduler
    daemon: simple
    plugs:
    - network-bind
  daemon-kubelet:
    command: run-with-config-args kubelet
    daemon: simple
    plugs:
    - dot-kube
    - firewall-control
    - network-bind
    - k8s-kubelet
    - hardware-observe
    - mount-observe
    - network-control
    - process-control
    - system-observe
  daemon-proxy:
    command: run-with-config-args kube-proxy
    daemon: simple
    plugs:
    - network-bind
    - firewall-control
    - k8s-kubeproxy
    - kernel-module-observe
    - mount-observe
    - network-observe
    - system-observe
    - kernel-module-control
  kubectl:
    command: microk8s-kubectl.wrapper
    completer: kubectl.bash
    plugs:
    - dot-kube
    - firewall-control
    - network-bind
    - k8s-kubelet
    - hardware-observe
    - mount-observe
    - network-control
    - process-control
    - system-observe
  add-node:
    command: microk8s-add-node.wrapper
  join:
    command: microk8s-join.wrapper
  remove-node:
    command: microk8s-remove-node.wrapper
  leave:
    command: microk8s-leave.wrapper
  ctr:
    command: microk8s-ctr.wrapper
  inspect:
    command: inspect.sh
  enable:
    command: microk8s-enable.wrapper
    plugs:
    - network
    - network-control
  disable:
    command: microk8s-disable.wrapper
    plugs:
    - network
    - network-control
  start:
    command: microk8s-start.wrapper
    plugs:
    - network
  stop:
    command: microk8s-stop.wrapper
    plugs:
    - network
  status:
    command: microk8s-status.wrapper
    plugs:
    - network
  config:
    command: microk8s-config.wrapper
  reset:
    command: microk8s-reset.wrapper
    plugs:
    - network
  istioctl:
    command: microk8s-istioctl.wrapper
    plugs:
    - network
  linkerd:
    command: microk8s-linkerd.wrapper
    plugs:
    - network
  helm:
    command: microk8s-helm.wrapper
    plugs:
    - network
  cilium:
    command: microk8s-cilium.wrapper
    plugs:
    - network

passthrough:
  layout:
    /usr/libexec:
      symlink: $SNAP_COMMON/usr/libexec
    /var/lib/cni:
      bind: $SNAP_COMMON/var/lib/cni
    /var/log/pods:
      bind: $SNAP_COMMON/var/log/pods
    /var/log/containers:
      bind: $SNAP_COMMON/var/log/containers
    /var/lib/kubelet:
      bind: $SNAP_DATA/kubelet
    /var/lib/kube-proxy:
      bind: $SNAP_DATA/kube-proxy

parts:
  libnftnl:
    plugin: autotools
    source: https://www.netfilter.org/projects/libnftnl/files/libnftnl-1.0.9.tar.bz2
    build-packages:
    - libjansson-dev
    - libmnl-dev
  iptables:
    after:
    - libnftnl
    source: https://www.netfilter.org/projects/iptables/files/iptables-1.6.1.tar.bz2
    plugin: autotools
    build-packages:
    - bison
    - flex
    - libmnl-dev
    - libnfnetlink-dev
    - libnetfilter-conntrack3
    - libnetfilter-conntrack-dev
    configflags:
    - "--disable-shared"
    - "--enable-static"
    prime: [ -bin/iptables-xml ]
  go:
    source-tag: go1.12.7
  containerd:
    after: [go, iptables]
    source: https://github.com/containerd/containerd
    source-type: git
    plugin: go
    go-importpath: github.com/containerd/containerd
    build-packages:
    - btrfs-tools
    - libseccomp-dev
    override-build: |
      set -eu
      . ../../../build-scripts/set-env-variables.sh

      go version
      export GOPATH=$(realpath ../go)
      mkdir -p $SNAPCRAFT_PART_INSTALL/bin
      # Build containerd
      go get github.com/containerd/containerd
      (
        cd $GOPATH/src/github.com/containerd/containerd
        git checkout ${CONTAINERD_COMMIT}
        # building the btrfs driver can be disabled via the
        # build tag no_btrfs, removing this dependency
        make
      )
      cp $GOPATH/src/github.com/containerd/containerd/bin/* $SNAPCRAFT_PART_INSTALL/bin/
      rm $SNAPCRAFT_PART_INSTALL/bin/containerd-stress

      # Build runc
      go get github.com/opencontainers/runc
      (
        cd $GOPATH/src/github.com/opencontainers/runc
        git checkout ${RUNC_COMMIT}
        make BUILDTAGS='seccomp apparmor'
      )
      cp $GOPATH/src/github.com/opencontainers/runc/runc $SNAPCRAFT_PART_INSTALL/bin/

      # Assemble the snap
      # snapcraftctl build
    organize:
      containerd/install/bin/*: bin/
    stage-packages:
    - libnss-myhostname
    - libnss-resolve
    - libnss-mymachines
    - conntrack
    - curl
    - aufs-tools
    - gawk
    - sed
    - socat
    - grep
    - libssl1.0.0
    - coreutils
    - hostname
    - diffutils    
    - vim
    stage:
    - -sbin/xtables-multi
    - -sbin/iptables*
    - -lib/xtables
  cluster-agent:
    plugin: python
    python-version: python3
    source: .
    python-packages:
    - flask
    stage-packages:
    - python3-openssl
    - openssl
    - python3-requests
    - gunicorn3
  microk8s:
    after: [containerd]
    plugin: dump
    build-attributes: [no-patchelf]
    build-packages:
    - make
    - mercurial
    - git
    - rsync
    - openssl
    - file
    - dpkg
    stage-packages:
    - net-tools
    - util-linux
    - zfsutils-linux
    - iproute2
    source: .
    prime:
      - -README*
      - -tests*
      - -docs*
      - -build*
    override-build: |
      set -eux
      . build-scripts/set-env-variables.sh

      REMOVE_KUBE_SNAP_BINS=false
      echo "Reached here"
      if [ ! -e "${KUBE_SNAP_BINS}" ]; then
        if [ -z "${KUBE_SNAP_BINS}" ]; then
          REMOVE_KUBE_SNAP_BINS=true
          . build-scripts/set-env-binaries-location.sh
        fi
        echo "Downloading binaries"
        . build-scripts/fetch-other-binaries.sh
        if [ -z "${KUBERNETES_COMMIT}" ]; then
          echo "Downloading k8s binaries from upstream"
          . build-scripts/fetch-k8s-binaries.sh
        else
          echo "Building k8s binaries"
          . build-scripts/build-k8s-binaries.sh
        fi
      else
        echo "Binaries provided in $KUBE_SNAP_BINS"
      fi

      echo "Setting default daemon configs"
      cp -r $KUBE_SNAP_ROOT/microk8s-resources/default-args .

      echo "Building certs"
      cp -r $KUBE_SNAP_ROOT/microk8s-resources/certs .
      cp -r $KUBE_SNAP_ROOT/microk8s-resources/certs-beta .

      echo "Preparing cni"
      mkdir -p opt/cni/bin/
      cp $KUBE_SNAP_BINS/cni/* opt/cni/bin/

      echo "Preparing flanneld"
      mkdir -p opt/cni/bin/
      cp $KUBE_SNAP_BINS/flanneld/flanneld opt/cni/bin/

      echo "Preparing containerd"
      cp $KUBE_SNAP_ROOT/microk8s-resources/containerd-profile .

      echo "Preparing etcd"
      cp $KUBE_SNAP_BINS/etcd/etcd .
      cp $KUBE_SNAP_BINS/etcd/etcdctl .

      echo "Preparing kube-apiserver"
      cp $KUBE_SNAP_BINS/$KUBE_ARCH/kube-apiserver .
      # Old versions will be pointing to these .csv files from inside their kube-apiserver config
      # Keep them around for a couple of releases.
      touch known_token.csv
      cp $KUBE_SNAP_ROOT/microk8s-resources/basic_auth.csv .

      echo "Preparing kube-controller-manager"
      cp $KUBE_SNAP_BINS/$KUBE_ARCH/kube-controller-manager .

      echo "Preparing kube-scheduler"
      cp $KUBE_SNAP_BINS/$KUBE_ARCH/kube-scheduler .

      echo "Preparing kubelet"
      mkdir -p configs
      cp $KUBE_SNAP_BINS/$KUBE_ARCH/kubelet .

      echo "Preparing kube-proxy"
      cp $KUBE_SNAP_BINS/$KUBE_ARCH/kube-proxy .

      echo "Preparing kubelet"
      cp $KUBE_SNAP_BINS/$KUBE_ARCH/kubectl .

      echo "Preparing user config"
      cp $KUBE_SNAP_ROOT/microk8s-resources/client.config.template .

      echo "Creating commands and wrappers"
      cp $KUBE_SNAP_ROOT/microk8s-resources/wrappers/* .

      cp -r $KUBE_SNAP_ROOT/microk8s-resources/actions .
      if [ "${ARCH}" = "arm64" ]
      then
        # Some actions are not available on arm64
        # Nvidia support
        rm "actions/enable.gpu.sh"
        rm "actions/disable.gpu.sh"
        rm "actions/gpu.yaml"
        # Istio support
        rm "actions/enable.istio.sh"
        rm "actions/disable.istio.sh"
        # Knative support
        rm "actions/enable.knative.sh"
        rm "actions/disable.knative.sh"
        # Prometheus support
        rm "actions/enable.prometheus.sh"
        rm "actions/disable.prometheus.sh"
        rm -rf "actions/prometheus"
        # Fluentd support
        rm "actions/enable.fluentd.sh"
        rm "actions/disable.fluentd.sh"
        rm -rf "actions/fluentd"
        # Jeager support
        rm "actions/enable.jaeger.sh"
        rm "actions/disable.jaeger.sh"
        rm -rf "actions/jaeger"
        # Linkerd support
        rm "actions/enable.linkerd.sh"
        rm "actions/disable.linkerd.sh"
      else
        # Knative support
        echo "Preparing knative"
        cp -r $KUBE_SNAP_BINS/knative-yaml ./actions/knative
      fi

      echo "Creating inspect hook"
      cp $KUBE_SNAP_ROOT/scripts/inspect.sh .

      # Add bash completion for microk8s.kubectl. 
      ./kubectl completion bash | sed "s/complete -o default -F __start_kubectl kubectl/complete -o default -F __start_kubectl microk8s.kubectl/g" | sed "s/complete -o default -o nospace -F __start_kubectl kubectl/complete -o default -o nospace -F __start_kubectl kubectl/g" > kubectl.bash

      if $REMOVE_KUBE_SNAP_BINS; then
        rm -rf "$KUBE_SNAP_BINS"
      fi
      snapcraftctl build

  # Unfortunately we cannot add package repositories to our snaps
  # https://forum.snapcraft.io/t/proposal-additional-package-sources/2199
  # We handpick the debs we need.
  # To update these debs add the repository
  # Follow the instructions in https://github.com/NVIDIA/nvidia-docker and
  # install the https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list repo.
  # Use apt-cache show <package> to make sure
  # a) the dockerd we ship in this snap is supported by nvidia-container-runtime
  # b) the version dependencies of each package is met.
  nvidia-runtime:
    plugin: dump
    source: https://nvidia.github.io/nvidia-container-runtime/ubuntu16.04/amd64/nvidia-container-runtime_2.0.0+docker18.06.1-1_amd64.deb
    source-type: deb
    override-build: |
      set -eu
      ARCH=$(dpkg --print-architecture)
      if ! [ "${ARCH}" = "arm64" ]
      then
        snapcraftctl build
      else
        echo "Skipped"
      fi

  nvidia-runtime-hook:
    plugin: dump
    source: https://nvidia.github.io/nvidia-container-runtime/ubuntu16.04/amd64/nvidia-container-runtime-hook_1.4.0-1_amd64.deb
    source-type: deb
    override-build: |
      set -eu
      ARCH=$(dpkg --print-architecture)
      if ! [ "${ARCH}" = "arm64" ]
      then
        snapcraftctl build
      else
        echo "Skipped"
      fi

  libnvidia:
    plugin: dump
    source: https://nvidia.github.io/libnvidia-container/ubuntu16.04/amd64/libnvidia-container1_1.0.0-1_amd64.deb
    source-type: deb
    override-build: |
      set -eu
      ARCH=$(dpkg --print-architecture)
      if ! [ "${ARCH}" = "arm64" ]
      then
        snapcraftctl build
      else
        echo "Skipped"
      fi

  libnvidia-tools:
    plugin: dump
    source: https://nvidia.github.io/libnvidia-container/ubuntu16.04/amd64/libnvidia-container-tools_1.0.0-1_amd64.deb
    source-type: deb
    override-build: |
      set -eu
      ARCH=$(dpkg --print-architecture)
      if ! [ "${ARCH}" = "arm64" ]
      then
        snapcraftctl build
      else
        echo "Skipped"
      fi

